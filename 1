\documentclass[a4paper,11pt]{article}

% ---------- Encoding & typography ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[stretch=10]{microtype}
\usepackage{parskip} % no indents, space between paragraphs

% ---------- Math & units ----------
\usepackage{amsmath, amssymb, mathtools, bm}
\usepackage{siunitx}
\sisetup{detect-all=true}

% ---------- Graphics & floats (not really needed here, but harmless) ----------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

% ---------- Links & references ----------
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink,capitalise]{cleveref}

% ---------- Code (optional) ----------
\usepackage[cache=false]{minted}
\setminted{
  fontsize=\small,
  breaklines=true,
  autogobble=true,
  frame=lines,
  framesep=2mm
}

% ---------- Page layout ----------
\usepackage[margin=1in]{geometry}

% ---------- Handy macros ----------
\newcommand{\R}{\mathbb{R}}
\newcommand{\abs}[1]{\left|#1\right|}

\title{The Chain Rule and the Idea of Backpropagation}
\author{Group 2 -- Ferschl Martin, Reiter Roman, Zenkic Mirza}
\date{\today}

\begin{document}
\maketitle

\section{Task specification}

Let \(f(x)\) be a function that depends on two parameters \(a\) and \(b\). More
precisely, we write
\[
  f = f(x; a,b),
\]
so \(f\) is a function in three variables. Furthermore, let
\[
  F(x) = f\bigl(f(x + a; a,b) + b \,;\, a,b\bigr).
\]

The exercise asks us to:
\begin{enumerate}
  \item Compute the partial derivatives \(F_a = \frac{\partial F}{\partial a}\)
        and \(F_b = \frac{\partial F}{\partial b}\) in terms of the corresponding
        partial derivatives of \(f\) (using the chain rule).
  \item Observe that the expression for \(F_b\) also appears in \(F_a\), so that
        it is natural to compute \(F_b\) first and reuse intermediate results.
  \item Write the derivation down precisely and explain the procedure
        (backpropagation viewpoint).
\end{enumerate}

\section{Setup and notation}

To apply the chain rule systematically, we introduce intermediate variables
describing the ``layers'' of the composition. This is exactly the computational
graph perspective used in backpropagation.

We regard \(f\) as a function of three arguments, and we denote its partial
derivatives by
\[
  f_x(x;a,b) := \frac{\partial f}{\partial x}(x;a,b), \qquad
  f_a(x;a,b) := \frac{\partial f}{\partial a}(x;a,b), \qquad
  f_b(x;a,b) := \frac{\partial f}{\partial b}(x;a,b).
\]

Now introduce the following intermediate quantities:
\begin{align*}
  z &= x + a, \\
  y_{\text{inner}} &= f(z; a,b), \\
  y_{\text{outer}} &= y_{\text{inner}} + b
                  = f(z; a,b) + b, \\
  F(x) &= f\bigl(y_{\text{outer}}; a,b\bigr).
\end{align*}

Thus the computational graph is
\[
  x \xrightarrow{+a} z \xrightarrow{f(\cdot;a,b)} y_{\text{inner}}
    \xrightarrow{+b} y_{\text{outer}} \xrightarrow{f(\cdot;a,b)} F.
\]

We will compute the gradients by starting from the output (outermost layer) and
propagating derivatives backwards.

\section{Computation of \texorpdfstring{$F_b$}{Fb}}

We first compute the partial derivative of \(F\) with respect to \(b\). The
parameter \(b\) influences \(F\) through three distinct paths:
\begin{enumerate}
  \item directly as a parameter of the outer \(f\),
  \item explicitly through the term \(+b\) in \(y_{\text{outer}} = y_{\text{inner}} + b\),
  \item implicitly as a parameter in the inner \(f(z;a,b)\).
\end{enumerate}

Applying the chain rule to the outermost function,
\[
  F(x) = f\bigl(y_{\text{outer}}; a,b\bigr),
\]
gives
\[
  F_b
  = \frac{\partial F}{\partial b}
  = f_x\bigl(y_{\text{outer}};a,b\bigr) \cdot
      \frac{\partial y_{\text{outer}}}{\partial b}
    + f_b\bigl(y_{\text{outer}};a,b\bigr).
\]

We now compute \(\frac{\partial y_{\text{outer}}}{\partial b}\).
Since
\[
  y_{\text{outer}} = f(z; a,b) + b
\]
with \(z = x + a\) independent of \(b\), we obtain
\[
  \frac{\partial y_{\text{outer}}}{\partial b}
  = \frac{\partial}{\partial b}\bigl(f(z; a,b)\bigr) + \frac{\partial b}{\partial b}
  = f_b(z; a,b) + 1.
\]

Substituting back, we obtain
\[
  F_b
  = f_x\bigl(y_{\text{outer}};a,b\bigr)\,\bigl(f_b(z; a,b) + 1\bigr)
    + f_b\bigl(y_{\text{outer}};a,b\bigr).
\]

This already has the structure typical for backpropagation: the factor
\(f_x(y_{\text{outer}};a,b)\) is the ``sensitivity'' of the output with respect
to its input, and it will be reused in the derivative with respect to other
parameters.

\section{Computation of \texorpdfstring{$F_a$}{Fa}}

Next we compute \(F_a = \frac{\partial F}{\partial a}\). The parameter \(a\)
influences \(F\) through:
\begin{enumerate}
  \item directly as a parameter of the outer \(f\),
  \item implicitly through its role in the inner function \(f(z; a,b)\),
  \item implicitly via \(z = x + a\).
\end{enumerate}

Again apply the chain rule at the outermost level:
\[
  F_a
  = \frac{\partial F}{\partial a}
  = f_x\bigl(y_{\text{outer}};a,b\bigr) \cdot
      \frac{\partial y_{\text{outer}}}{\partial a}
    + f_a\bigl(y_{\text{outer}};a,b\bigr).
\]

We already know that the factor \(f_x(y_{\text{outer}};a,b)\) appears in \(F_b\),
so we do not need to recompute it. It is reused here, exactly as in
backpropagation.

Now we compute \(\frac{\partial y_{\text{outer}}}{\partial a}\). Since
\(y_{\text{outer}} = f(z; a,b) + b\) and \(b\) does not depend on \(a\),
\[
  \frac{\partial y_{\text{outer}}}{\partial a}
  = \frac{\partial}{\partial a} f(z; a,b).
\]
Here we must apply the chain rule again, because \(z = x + a\) depends on \(a\):
\[
  \frac{\partial}{\partial a} f(z; a,b)
  = f_x(z; a,b) \cdot \frac{\partial z}{\partial a}
    + f_a(z; a,b).
\]
Since \(\frac{\partial z}{\partial a} = 1\), we obtain
\[
  \frac{\partial y_{\text{outer}}}{\partial a}
  = f_x(z; a,b) + f_a(z; a,b).
\]

Substituting into the expression for \(F_a\) gives
\[
  F_a
  = f_x\bigl(y_{\text{outer}};a,b\bigr)\,\bigl(f_x(z; a,b) + f_a(z; a,b)\bigr)
    + f_a\bigl(y_{\text{outer}};a,b\bigr).
\]

\section{Summary and backpropagation viewpoint}

For compactness, write
\[
  z = x + a, \qquad
  y = f(z; a,b) + b
\]
and, to avoid clutter, suppress the explicit parameters \((a,b)\) in the
notation of the derivatives (it is understood that all \(f_x, f_a, f_b\) are
evaluated at the appropriate argument together with the same parameters
\((a,b)\)). Then we can summarise:
\begin{align*}
  F_b(x;a,b)
  &= f_x(y)\,\bigl(f_b(z) + 1\bigr) + f_b(y), \\
  F_a(x;a,b)
  &= f_x(y)\,\bigl(f_x(z) + f_a(z)\bigr) + f_a(y).
\end{align*}

The procedure illustrates the principle of \emph{backpropagation}:
\begin{itemize}
  \item We express the computation as a sequence of simple steps (a
        computational graph).
  \item We compute derivatives layer by layer, starting from the output and
        moving backwards.
  \item Intermediate ``sensitivities'' such as \(f_x(y)\) are computed once and
        reused when differentiating with respect to different parameters (here
        both \(a\) and \(b\)).
\end{itemize}
This is exactly how gradient computation is implemented efficiently in neural
networks.

\end{document}

% %========================= expm1-report.tex =========================
% \documentclass[a4paper,11pt]{article}
% 
% % ---------- Encoding & typography ----------
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{lmodern}
% \usepackage[stretch=10]{microtype}
% \usepackage{parskip} % no indents, space between paragraphs
% 
% % ---------- Math & units ----------
% \usepackage{amsmath, amssymb, mathtools, bm}
% \usepackage{siunitx}
% \sisetup{detect-all=true}
% 
% % ---------- Graphics & floats ----------
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{xcolor}
% 
% % ---------- Links & references ----------
% \usepackage[hidelinks]{hyperref}
% \usepackage[nameinlink,capitalise]{cleveref}
% 
% % ---------- Code (choose ONE of the two) ----------
% % Option A: minted (better highlighting) — compile with: -shell-escape
% \usepackage[cache=false]{minted}
% \setminted{
%   fontsize=\small,
%   breaklines=true,
%   autogobble=true,
%   frame=lines,
%   framesep=2mm
% }
% 
% % % Option B: listings (no shell-escape required)
% % \usepackage{listings}
% % \lstset{
% %   language=Python,
% %   basicstyle=\ttfamily\small,
% %   numbers=left,
% %   numbersep=6pt,
% %   breaklines=true,
% %   frame=lines
% % }
% 
% % ---------- Page layout ----------
% \usepackage[margin=1in]{geometry}
% 
% % ---------- Handy macros ----------
% \newcommand{\R}{\mathbb{R}}
% \newcommand{\abs}[1]{\left|#1\right|}
% \DeclareMathOperator{\expmone}{expm1}
% 
% \title{The chain rule and the idea of ‘backpropagation’.}
% \author{Group 2 - Ferschl Martin, Laktaoui Mohammed, Reiter Roman, Zenkic Mirza}
% \date{\today}
% 
% \begin{document}
% \maketitle
% 
% % \begin{abstract}
% % We study the numerical evaluation of \(f(x)=\exp(x)-1\) for very small \(\abs{x}\).
% % We compare the direct evaluation in double precision against a truncated Taylor
% % series about \(x=0\) and visualize the relative error on a logarithmic scale.
% % We explain the loss of accuracy due to catastrophic cancellation and give a
% % stable alternative.
% % \end{abstract}
% 
% \section{Task specification}
% 
% Let $f(x)$ be a function that depends on two parameters $a$ and $b$. (More
% precisely, $f = f(x; a,b)$, meaning that $f$ is actually a function in $3$ variables.)
% Furthermore, let $F(x) = f(f(x+ a) + b)$.
% 
% \begin{enumerate}
%     \item Compute the partial derivatives $F_a$ and $F_b$ of $F$ with respect to $a$ and $b$ in terms of the corresponding derivatives of $f$. (This requires use of the \textbf{Chain Rule}).
%     \item Note that the expression for the partial derivative $F_b$ also appears in the derivative $F_a$. Plan to first compute $F_b$ and then substitute the result into $F_a$.
%     \item Write everything down precisely and explain the procedure.
% \end{enumerate}
% 
% \section{Procedure and Definitions}
% To apply the chain rule precisely we must define the intermediate variables representing the \textquotedblleft layers\textquotedblright{} of this composition. This approach mirrors the \textquotedblleft backpropagation\textquotedblright{} method used in neural networks, where derivatives are computed layer by layer.
% 
% Let us define the partial derivatives of the function $f(u;a,b)$ with respect to its three arguments:
% \begin{itemize}
%     \item $f_u(u;a,b)=\frac{\partial f}{\partial u}$ (Derivative with respect to the first argument/input).
%     \item $f_a(u;a,b)=\frac{\partial f}{\partial a}$ (Derivative with respect to parameter $a$).
%     \item $f_b(u;a,b)=\frac{\partial f}{\partial b}$ (Derivative with respect to parameter $b$).
% \end{itemize}
% 
% \textbf{Step-by-Step Decomposition:}
% \begin{enumerate}
%     \item \textbf{Inner Layer Input:} Let $z=x+a$.
%     \item \textbf{Inner Layer Output:} Let $y_{\text{inner}}=f(z;a,b)$.
%     \item \textbf{Outer Layer Input:} Let $y_{\text{outer}}=y_{\text{inner}}+b$.
%     \item \textbf{Final Output:} $F=f(y_{\text{outer}};a,b)$.
% \end{enumerate}
% We will compute the derivatives by moving from the outer layer inward.
% 
% % \hrule
% 
% \section{Computation of $F_b$}
% We seek $\frac{\partial F}{\partial b}$. The parameter $b$ influences $F$ through three distinct paths:
% \begin{enumerate}
%     \item Directly as a parameter in the outer function $f$.
%     \item Explicitly via the term $+b$ in the argument $y_{\text{outer}}$.
%     \item Implicitly via the parameter $b$ inside the inner function $y_{\text{inner}}$.
% \end{enumerate}
% 
% Using the chain rule:
% $$F_b = \frac{\partial F}{\partial b} = \underbrace{\frac{\partial f}{\partial u}(y_{\text{outer}};a,b)}_{\text{Outer derivative}} \cdot \frac{\partial y_{\text{outer}}}{\partial b} + \underbrace{\frac{\partial f}{\partial b}(y_{\text{outer}};a,b)}_{\text{Direct parameter derivative}}$$
% 
% Now we compute $\frac{\partial y_{\text{outer}}}{\partial b}$. Since $y_{\text{outer}}=f(z;a,b)+b$:
% $$\frac{\partial y_{\text{outer}}}{\partial b} = \frac{\partial}{\partial b}(f(z;a,b)+b) = f_b(z;a,b) + 1$$
% 
% Substituting this back:
% $$F_b = f_u(y_{\text{outer}};a,b) \cdot \left[f_b(z;a,b) + 1\right] + f_b(y_{\text{outer}};a,b)$$
% This establishes the derivative with respect to $b$. Note that the term $f_u(y_{\text{outer}};a,b)$ represents the sensitivity of the outer layer to its input, often denoted as the error signal or $\delta$ in backpropagation.
% 
% % \hrule
% 
% \section{Computation of $F_a$}
% We seek $\frac{\partial F}{\partial a}$. The parameter $a$ influences $F$ through three paths:
% \begin{enumerate}
%     \item Directly as a parameter in the outer function $f$.
%     \item Implicitly via the parameter $a$ inside the inner function $y_{\text{inner}}$.
%     \item Implicitly via the term $+a$ in the innermost input $z$.
% \end{enumerate}
% 
% Using the chain rule:
% $$F_a = \frac{\partial F}{\partial a} = \underbrace{\frac{\partial f}{\partial u}(y_{\text{outer}};a,b)}_{\text{Outer derivative}} \cdot \frac{\partial y_{\text{outer}}}{\partial a} + \underbrace{\frac{\partial f}{\partial a}(y_{\text{outer}};a,b)}_{\text{Direct parameter derivative}}$$
% 
% Notice that the first term, $f_u(y_{\text{outer}};a,b)$, is the exact same factor we computed for $F_b$. This confirms the note in the exercise: we can reuse this computed value.
% 
% Now we compute $\frac{\partial y_{\text{outer}}}{\partial a}$. Since $y_{\text{outer}}=f(z;a,b)+b$ (and $b$ is constant w.r.t $a$):
% $$\frac{\partial y_{\text{outer}}}{\partial a} = \frac{\partial}{\partial a} f(z;a,b)$$
% 
% To differentiate the inner function $f(z;a,b)$ with respect to $a$, we must apply the chain rule again because $z=x+a$ depends on $a$:
% $$\frac{\partial}{\partial a} f(z;a,b) = f_u(z;a,b) \cdot \frac{\partial z}{\partial a} + f_a(z;a,b)$$
% Since $z=x+a$, $\frac{\partial z}{\partial a}=1$. Thus:
% $$\frac{\partial y_{\text{outer}}}{\partial a} = f_u(z;a,b) \cdot (1) + f_a(z;a,b) = f_u(z;a,b) + f_a(z;a,b)$$
% 
% Substituting this back:
% $$F_a = f_u(y_{\text{outer}};a,b) \cdot \left[f_u(z;a,b) + f_a(z;a,b)\right] + f_a(y_{\text{outer}};a,b)$$
% 
% % \hrule
% 
% \section{Summary of Results}
% Using the notation $z=x+a$ and $y=f(x+a;a,b)+b$ (where $y$ is shorthand for $y_{\text{outer}}$):
% \begin{enumerate}
%     \item \textbf{Partial Derivative $F_b$:}
%     $$F_b = f_u(y) \cdot \left[f_b(z) + 1\right] + f_b(y)$$
%     \item \textbf{Partial Derivative $F_a$:}
%     $$F_a = f_u(y) \cdot \left[f_u(z) + f_a(z)\right] + f_a(y)$$
% \end{enumerate}
% \textbf{Explanation of the Procedure:} The procedure demonstrates the principle of \textbf{backpropagation}. We calculated the derivative of the outermost layer first. The term $f_u(y)$ appears in the calculation for both $F_a$ and $F_b$. In a computational graph, this value would be computed once and propagated backwards to determine how the changes in parameters $a$ and $b$ affect the inner components and, ultimately, the final output. By substituting the previously calculated sensitivity of the outer layer, we efficiently derive the gradients for all parameters.
% 
% % \section{Method}
% 
% % \section{Implementation}
% 
% % % ---------- If using listings, replace the minted block above with:
% % \begin{lstlisting}
% % # (same Python code as above)
% % \end{lstlisting}
% 
% % \section{Results}
% 
% % \section{Discussion}
% 
% % (Optional) short references could go here, inline or as a mini list.
% 
% % \appendix
% % \section{Taylor polynomial (degree 10)}
% % \[
% %   \exp(x)-1 \approx x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^{10}}{10!}.
% % \]
% 
% \end{document}
% %======================= end expm1-report.tex =======================
% 
