%========================= expm1-report.tex =========================
\documentclass[a4paper,11pt]{article}

% ---------- Encoding & typography ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[stretch=10]{microtype}
\usepackage{parskip} % no indents, space between paragraphs

% ---------- Math & units ----------
\usepackage{amsmath, amssymb, mathtools, bm}
\usepackage{siunitx}
\sisetup{detect-all=true}

% ---------- Graphics & floats ----------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

% ---------- Links & references ----------
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink,capitalise]{cleveref}

% ---------- Code (choose ONE of the two) ----------
% Option A: minted (better highlighting) — compile with: -shell-escape
\usepackage[cache=false]{minted}
\setminted{
  fontsize=\small,
  breaklines=true,
  autogobble=true,
  frame=lines,
  framesep=2mm
}

% % Option B: listings (no shell-escape required)
% \usepackage{listings}
% \lstset{
%   language=Python,
%   basicstyle=\ttfamily\small,
%   numbers=left,
%   numbersep=6pt,
%   breaklines=true,
%   frame=lines
% }

% ---------- Page layout ----------
\usepackage[margin=1in]{geometry}

% ---------- Handy macros ----------
\newcommand{\R}{\mathbb{R}}
\newcommand{\abs}[1]{\left|#1\right|}
\DeclareMathOperator{\expmone}{expm1}

\title{The chain rule and the idea of ‘backpropagation’.}
\author{Group 2 - Ferschl Martin, Laktaoui Mohammed, Reiter Roman, Zenkic Mirza}
\date{\today}

\begin{document}
\maketitle

% \begin{abstract}
% We study the numerical evaluation of \(f(x)=\exp(x)-1\) for very small \(\abs{x}\).
% We compare the direct evaluation in double precision against a truncated Taylor
% series about \(x=0\) and visualize the relative error on a logarithmic scale.
% We explain the loss of accuracy due to catastrophic cancellation and give a
% stable alternative.
% \end{abstract}

\section{Task specification}

Let $f(x)$ be a function that depends on two parameters $a$ and $b$. (More
precisely, $f = f(x; a,b)$, meaning that $f$ is actually a function in $3$ variables.)
Furthermore, let $F(x) = f(f(x+ a) + b)$.

\begin{enumerate}
    \item Compute the partial derivatives $F_a$ and $F_b$ of $F$ with respect to $a$ and $b$ in terms of the corresponding derivatives of $f$. (This requires use of the \textbf{Chain Rule}).
    \item Note that the expression for the partial derivative $F_b$ also appears in the derivative $F_a$. Plan to first compute $F_b$ and then substitute the result into $F_a$.
    \item Write everything down precisely and explain the procedure.
\end{enumerate}

\section{Procedure and Definitions}
To apply the chain rule precisely we must define the intermediate variables representing the \textquotedblleft layers\textquotedblright{} of this composition. This approach mirrors the \textquotedblleft backpropagation\textquotedblright{} method used in neural networks, where derivatives are computed layer by layer.

Let us define the partial derivatives of the function $f(u;a,b)$ with respect to its three arguments:
\begin{itemize}
    \item $f_u(u;a,b)=\frac{\partial f}{\partial u}$ (Derivative with respect to the first argument/input).
    \item $f_a(u;a,b)=\frac{\partial f}{\partial a}$ (Derivative with respect to parameter $a$).
    \item $f_b(u;a,b)=\frac{\partial f}{\partial b}$ (Derivative with respect to parameter $b$).
\end{itemize}

\textbf{Step-by-Step Decomposition:}
\begin{enumerate}
    \item \textbf{Inner Layer Input:} Let $z=x+a$.
    \item \textbf{Inner Layer Output:} Let $y_{\text{inner}}=f(z;a,b)$.
    \item \textbf{Outer Layer Input:} Let $y_{\text{outer}}=y_{\text{inner}}+b$.
    \item \textbf{Final Output:} $F=f(y_{\text{outer}};a,b)$.
\end{enumerate}
We will compute the derivatives by moving from the outer layer inward.

% \hrule

\section{Computation of $F_b$}
We seek $\frac{\partial F}{\partial b}$. The parameter $b$ influences $F$ through three distinct paths:
\begin{enumerate}
    \item Directly as a parameter in the outer function $f$.
    \item Explicitly via the term $+b$ in the argument $y_{\text{outer}}$.
    \item Implicitly via the parameter $b$ inside the inner function $y_{\text{inner}}$.
\end{enumerate}

Using the chain rule:
$$F_b = \frac{\partial F}{\partial b} = \underbrace{\frac{\partial f}{\partial u}(y_{\text{outer}};a,b)}_{\text{Outer derivative}} \cdot \frac{\partial y_{\text{outer}}}{\partial b} + \underbrace{\frac{\partial f}{\partial b}(y_{\text{outer}};a,b)}_{\text{Direct parameter derivative}}$$

Now we compute $\frac{\partial y_{\text{outer}}}{\partial b}$. Since $y_{\text{outer}}=f(z;a,b)+b$:
$$\frac{\partial y_{\text{outer}}}{\partial b} = \frac{\partial}{\partial b}(f(z;a,b)+b) = f_b(z;a,b) + 1$$

Substituting this back:
$$F_b = f_u(y_{\text{outer}};a,b) \cdot \left[f_b(z;a,b) + 1\right] + f_b(y_{\text{outer}};a,b)$$
This establishes the derivative with respect to $b$. Note that the term $f_u(y_{\text{outer}};a,b)$ represents the sensitivity of the outer layer to its input, often denoted as the error signal or $\delta$ in backpropagation.

% \hrule

\section{Computation of $F_a$}
We seek $\frac{\partial F}{\partial a}$. The parameter $a$ influences $F$ through three paths:
\begin{enumerate}
    \item Directly as a parameter in the outer function $f$.
    \item Implicitly via the parameter $a$ inside the inner function $y_{\text{inner}}$.
    \item Implicitly via the term $+a$ in the innermost input $z$.
\end{enumerate}

Using the chain rule:
$$F_a = \frac{\partial F}{\partial a} = \underbrace{\frac{\partial f}{\partial u}(y_{\text{outer}};a,b)}_{\text{Outer derivative}} \cdot \frac{\partial y_{\text{outer}}}{\partial a} + \underbrace{\frac{\partial f}{\partial a}(y_{\text{outer}};a,b)}_{\text{Direct parameter derivative}}$$

Notice that the first term, $f_u(y_{\text{outer}};a,b)$, is the exact same factor we computed for $F_b$. This confirms the note in the exercise: we can reuse this computed value.

Now we compute $\frac{\partial y_{\text{outer}}}{\partial a}$. Since $y_{\text{outer}}=f(z;a,b)+b$ (and $b$ is constant w.r.t $a$):
$$\frac{\partial y_{\text{outer}}}{\partial a} = \frac{\partial}{\partial a} f(z;a,b)$$

To differentiate the inner function $f(z;a,b)$ with respect to $a$, we must apply the chain rule again because $z=x+a$ depends on $a$:
$$\frac{\partial}{\partial a} f(z;a,b) = f_u(z;a,b) \cdot \frac{\partial z}{\partial a} + f_a(z;a,b)$$
Since $z=x+a$, $\frac{\partial z}{\partial a}=1$. Thus:
$$\frac{\partial y_{\text{outer}}}{\partial a} = f_u(z;a,b) \cdot (1) + f_a(z;a,b) = f_u(z;a,b) + f_a(z;a,b)$$

Substituting this back:
$$F_a = f_u(y_{\text{outer}};a,b) \cdot \left[f_u(z;a,b) + f_a(z;a,b)\right] + f_a(y_{\text{outer}};a,b)$$

% \hrule

\section{Summary of Results}
Using the notation $z=x+a$ and $y=f(x+a;a,b)+b$ (where $y$ is shorthand for $y_{\text{outer}}$):
\begin{enumerate}
    \item \textbf{Partial Derivative $F_b$:}
    $$F_b = f_u(y) \cdot \left[f_b(z) + 1\right] + f_b(y)$$
    \item \textbf{Partial Derivative $F_a$:}
    $$F_a = f_u(y) \cdot \left[f_u(z) + f_a(z)\right] + f_a(y)$$
\end{enumerate}
\textbf{Explanation of the Procedure:} The procedure demonstrates the principle of \textbf{backpropagation}. We calculated the derivative of the outermost layer first. The term $f_u(y)$ appears in the calculation for both $F_a$ and $F_b$. In a computational graph, this value would be computed once and propagated backwards to determine how the changes in parameters $a$ and $b$ affect the inner components and, ultimately, the final output. By substituting the previously calculated sensitivity of the outer layer, we efficiently derive the gradients for all parameters.

% \section{Method}

% \section{Implementation}

% % ---------- If using listings, replace the minted block above with:
% \begin{lstlisting}
% # (same Python code as above)
% \end{lstlisting}

% \section{Results}

% \section{Discussion}

% (Optional) short references could go here, inline or as a mini list.

% \appendix
% \section{Taylor polynomial (degree 10)}
% \[
%   \exp(x)-1 \approx x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots + \frac{x^{10}}{10!}.
% \]

\end{document}
%======================= end expm1-report.tex =======================

